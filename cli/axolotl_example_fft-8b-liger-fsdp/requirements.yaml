version: "4"
dependencies:
  - packaging
  - setuptools
  - wheel
  - ninja
  - mlflow>=3.7

  # Axolotl with flash-attn and deepspeed (no build isolation)
  - --no-build-isolation
  - axolotl[flash-attn,deepspeed]==0.12.2

  # Flash attention wheel (no deps - specific CUDA/torch version)
  - --no-deps
  - https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp312-cp312-linux_x86_64.whl